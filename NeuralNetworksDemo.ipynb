{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "* What are NNs?\n",
    "* Why use NNs? Why not multivariable non-linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "* How can we normalize the data so that our ML model will perform better?\n",
    "* Generate good data that makes for easy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array([\n",
    "        [1,3],\n",
    "        [4,2],\n",
    "        [5,6]\n",
    "    ])\n",
    "targets = np.array([\n",
    "        [53],\n",
    "        [27],\n",
    "        [23]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "* Get a good diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Neural Network Architecture\n",
    "layer0Size = 2\n",
    "layer1Size = 3\n",
    "layer2Size = 1\n",
    "learningRate = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron\n",
    "\n",
    "Each neuron stores a vector whose size equals the number of incoming synapses. Neurons don’t store a single value. They store information about the cost to move to that neuron from every feature/input/synapse. So a layer is essentially a big matrix of dimensions equal to the rows of the incoming layer and the columns of the incoming weights matrix. It can be 3x1 or 3x3. Even in the final layer with a single neuron, the neuron will still store a vector (1D array) with a value for each incoming connection. This is important b/c we need a vector of derivatives (one for each direction/synapse) so we can start backprop optimization. The final output isn’t a single number (like it would be if we were making a prediction). If we wanted to make a prediction, we would sum up that final vector values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layer\n",
    "\n",
    "* Explain how neurons are connected in a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Layer 0 --> 1\n",
    "#Layer 0 has 2 columns\n",
    "#Layer 1 is of size 3\n",
    "#So our weights matrix must be size 2 x 3\n",
    "#Rows of input by Size of next layer\n",
    "l0_weights = np.array([\n",
    "        [.5,.5,.5],\n",
    "        [.5,.5,.5]        \n",
    "    ])\n",
    "\n",
    "#L1 dimensions = 3 x 3\n",
    "#L2 = size 1\n",
    "#L1 weights must be of 3 x 1\n",
    "#Rows of input by Size of next layer\n",
    "l1_weights = np.array([\n",
    "        [.5],\n",
    "        [.5],\n",
    "        [.5]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "* What is the purpose of the activation function? \n",
    "* Why do we use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    #Non-linear activation function..\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73105858,  0.95257413],\n",
       "       [ 0.98201379,  0.88079708],\n",
       "       [ 0.99330715,  0.99752738]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "sigmoid(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "* How does feed forward work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedForward(X, W):\n",
    "    '''\n",
    "    X - Previous Layer Matrix\n",
    "    W - Weights matrix connecting previous layer to current\n",
    "    Returns matrix representing neurons of current layer\n",
    "    with sigmoid activation function applied \n",
    "    '''\n",
    "    return sigmoid(np.dot(X,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1 Output\n",
      "[[ 0.88079708  0.88079708  0.88079708]\n",
      " [ 0.95257413  0.95257413  0.95257413]\n",
      " [ 0.99592986  0.99592986  0.99592986]]\n",
      "Layer2 Output\n",
      "[[ 0.78938056]\n",
      " [ 0.80672381]\n",
      " [ 0.81666214]]\n"
     ]
    }
   ],
   "source": [
    "a1 = feedForward(features,l0_weights)\n",
    "a2 = feedForward(a1,l1_weights)\n",
    "\n",
    "print \"Layer1 Output\"\n",
    "print a1\n",
    "print \"Layer2 Output\"\n",
    "print a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "* Which cost function do we use? And Why?\n",
    "* How is it different from cost functions in Linear/Logistic Regression?\n",
    "* Let's use MSE = 1/2(yHat-y)^2\n",
    "* The derivative of MSE is (yHat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650.68949642533767"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(predictions, targets):\n",
    "    '''\n",
    "    Predictions: N x M matrix\n",
    "    Targets: M x K matrix\n",
    "    Returns average squared error across predictions\n",
    "    '''\n",
    "    N = len(targets)\n",
    "    \n",
    "    #Take the squared error of each row\n",
    "    sq_error = (predictions - targets)**2\n",
    "\n",
    "    #Return the mean sum squared error among predictions\n",
    "    return 1.0/(2*N) * sq_error.sum()\n",
    "\n",
    "cost_function(a2,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Our cost function == Cost(Sigmoid(Hypothesis(x)))\n",
    "# Chain Rule = \n",
    "# via Chain Rule = dC/S * dS/H * dH/x\n",
    "\n",
    "# So to calculate the gradient, we take the \n",
    "    1. Derivative of Hypothesis w repect to X \n",
    "    2. Derivative of Sigmoid(Hypothesis)\n",
    "    3. Derivative of Cost(Sigmoid(Hypothesis)))\n",
    "    4. Multiply them all together\n",
    "    \n",
    "Derivative of Hypothesis = h(x) = W1*x1 + W2*x2 = W1+W2  =  constant (e.g. d/dx of 2x is 2)\n",
    "Derivative of Sigmoid = s(x) = x * (1-x)  =  h(x)*(1-h(x))\n",
    "Derivative of Cost = x(y-h(x))  =  s(x)(y-h(x))  =  h(x)*(1-h(x)) * (y-h(x))\n",
    "\n",
    "Sweet!!!!!!!!\n",
    "'''\n",
    "def costDeriv(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "'''\n",
    "One of the desirable properties of the sigmoid function is that its output \n",
    "can be used to create its derivative. If the sigmoid's output \n",
    "is x, then its derivative is x*(1-x). \n",
    "http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "'''\n",
    "def sigmoidDeriv(X):\n",
    "    '''\n",
    "    X - Matrix representing layer of neurons\n",
    "    Modifies values in matrix to derivative\n",
    "    \n",
    "    This is important because it's part of backprop\n",
    "    via the Chain Rule\n",
    "    \n",
    "    Hadamard element-wise multiplication\n",
    "    We are just modifying the matrix values in-place\n",
    "    The matrix dimensions remain the same\n",
    "    '''\n",
    "    return X * (1 - X)\n",
    "\n",
    "def lastLayerError(lastLayerWeightedInput, lastLayerOutput, targets):\n",
    "    '''\n",
    "    Should return a vector of the same dimensions as target\n",
    "    \n",
    "    l0_activation = 9x9\n",
    "    l1_activation = 3x1\n",
    "    targets = 3x1\n",
    "    \n",
    "    (lastLayerOutput-targets) * sigmoidDeriv(lastLayerWeightedInput)\n",
    "    (l1_activation - targets) * sigmoidDeriv(l0_activation)\n",
    "    [yHat]   [y]     \n",
    "    [yHat] - [y]  \n",
    "    [yHat]   [y]\n",
    "    \n",
    "    l1_activation - targets = 3x1\n",
    "    sigmoidDeriv(l0_activation) = 3x3\n",
    "    \n",
    "    x = hadamard product (element-wise multiplication)\n",
    "    \n",
    "    [yHat - y]     [S S S]     [E]\n",
    "    [yHat - y]  x  [S S S]  =  [E] \n",
    "    [yHat - y]     [S S S]     [E] \n",
    "    \n",
    "    '''\n",
    "    return (lastLayerOutput-targets) * sigmoidDeriv(lastLayerWeightedInput)\n",
    "\n",
    "def hiddenLayerError(hiddenLayerWeightedInput, weightsToNextLayer, nextLayerError):\n",
    "    return np.dot(nextLayerError, weightsToNextLayer.T) * sigmoidDeriv(hiddenLayerWeightedInput)\n",
    "\n",
    "def costGradientWithRespectToWeights(currentLayerError, currentLayerInput):\n",
    "    return np.dot(currentLayerInput.T, currentLayerError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Operation for multiplying weights + features and wrapping with sigmoid\n",
    "# Get formula\n",
    "#sigmoid(np.dot(features, weights))\n",
    "\n",
    "'''\n",
    "Above we had 2 incoming features and 3 neurons in the next layer\n",
    "\n",
    "We have 6 weights (synapses):\n",
    "    [.5,.5,.5],\n",
    "    [.5,.5,.5]\n",
    "\n",
    "The output looks like this:\n",
    "    [ 0.88079708  0.88079708  0.88079708]\n",
    "    [ 0.95257413  0.95257413  0.95257413]\n",
    "    [ 0.99592986  0.99592986  0.99592986]\n",
    " \n",
    " This really means:\n",
    " \n",
    "[[ sigmoid(f(W1,X))  sigmoid(f(W1,X))  sigmoid(f(W1,X))]\n",
    " [ sigmoid(f(W2,X))  sigmoid(f(W2,X))  sigmoid(f(W2,X))]\n",
    " [ sigmoid(f(W3,X))  sigmoid(f(W3,X))  sigmoid(f(W3,X))]]\n",
    " \n",
    "Given these are functions, we can take the partial derivatives to guide us in the correct direction to update each weight.\n",
    "'''\n",
    "print \"hey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[2],[2],[2]])\n",
    "b = np.array([\n",
    "        [3, 3, 3],\n",
    "        [3, 3, 3],\n",
    "        [3, 3, 3],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [4 2]\n",
      " [5 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,  -6],\n",
       "       [-12,  -2],\n",
       "       [-20, -30]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print features\n",
    "sigmoidDeriv(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, -3],\n",
       "       [-3, -4]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "        [1],[2]\n",
    "    ])\n",
    "b = np.array([\n",
    "        [3,4],\n",
    "        [5,6]\n",
    "    ])\n",
    "c = np.array([\n",
    "        [3,4],\n",
    "        [5,6],\n",
    "        [5,6],\n",
    "    ])\n",
    "a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.72750253  1.72750253  1.72750253]\n",
      " [ 1.87805752  1.87805752  1.87805752]] [[ 0.44889445]\n",
      " [ 0.44889445]\n",
      " [ 0.44889445]]\n",
      "[[ 13.2603347   13.2603347   13.2603347 ]\n",
      " [ 15.05160837  15.05160837  15.05160837]] [[ 0.40201064]\n",
      " [ 0.40201064]\n",
      " [ 0.40201064]]\n",
      "[[ 372.93232176  372.93232176  372.93232176]\n",
      " [ 428.04776666  428.04776666  428.04776666]] [[ 0.37699078]\n",
      " [ 0.37699078]\n",
      " [ 0.37699078]]\n",
      "[[ 162699.04072517  162699.04072517  162699.04072517]\n",
      " [ 187020.57553104  187020.57553104  187020.57553104]] [[ 0.36206975]\n",
      " [ 0.36206975]\n",
      " [ 0.36206975]]\n",
      "[[  1.88008649e+10   1.88008649e+10   1.88008649e+10]\n",
      " [  2.16139059e+10   2.16139059e+10   2.16139059e+10]] [[ 0.35263473]\n",
      " [ 0.35263473]\n",
      " [ 0.35263473]]\n",
      "[[  1.60006101e+20   1.60006101e+20   1.60006101e+20]\n",
      " [  1.83947138e+20   1.83947138e+20   1.83947138e+20]] [[ 0.34646167]\n",
      " [ 0.34646167]\n",
      " [ 0.34646167]]\n",
      "[[  7.61029309e+39   7.61029309e+39   7.61029309e+39]\n",
      " [  8.74895098e+39   8.74895098e+39   8.74895098e+39]] [[ 0.34233596]\n",
      " [ 0.34233596]\n",
      " [ 0.34233596]]\n",
      "[[  1.15272961e+79   1.15272961e+79   1.15272961e+79]\n",
      " [  1.32519737e+79   1.32519737e+79   1.32519737e+79]] [[ 0.33954029]\n",
      " [ 0.33954029]\n",
      " [ 0.33954029]]\n",
      "[[  1.79388638e+157   1.79388638e+157   1.79388638e+157]\n",
      " [  2.06227714e+157   2.06227714e+157   2.06227714e+157]] [[ 0.33762843]\n",
      " [ 0.33762843]\n",
      " [ 0.33762843]]\n",
      "[[ inf  inf  inf]\n",
      " [ inf  inf  inf]] [[ 0.33631287]\n",
      " [ 0.33631287]\n",
      " [ 0.33631287]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brendan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: RuntimeWarning: overflow encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "def update_weights(features, l0_weights, l1_weights, learningRate=.01):\n",
    "\n",
    "    # Feed Forward\n",
    "    '''\n",
    "    [1,3]                       [1 1 1]\n",
    "    [4,2]  dot  [.5 .5 .5 ]  =  [1 1 1] \n",
    "    [5,6]       [.5 .5 .5 ]     [1 1 1]\n",
    "\n",
    "    Then apply Sigmoid()\n",
    "    \n",
    "    10_activation = 9x9\n",
    "    '''\n",
    "    l0_activation = feedForward(features, l0_weights)\n",
    "\n",
    "    '''\n",
    "    [1 1 1]       [.5 ]     [2]\n",
    "    [1 1 1]  dot  [.5 ]  =  [2] \n",
    "    [1 1 1]       [.5 ]     [2]\n",
    "    \n",
    "    l1_activation = 3x1\n",
    "    '''\n",
    "    l1_activation = feedForward(l0_activation, l1_weights)\n",
    "    \n",
    "    \n",
    "    ### Backpropagate ###\n",
    "    \n",
    "    # Calculate error in each layer\n",
    "    '''\n",
    "    l0_activation = 9x9\n",
    "    l1_activation = 3x1\n",
    "    targets = 3x1\n",
    "    \n",
    "    (lastLayerOutput-targets) * sigmoidDeriv(lastLayerInput)\n",
    "    (l1_activation - targets) * sigmoidDeriv(l0_activation)\n",
    "    [yHat]   [y]     \n",
    "    [yHat] - [y]  \n",
    "    [yHat]   [y]\n",
    "    \n",
    "    l1_activation - targets = 3x1\n",
    "    sigmoidDeriv(l0_activation) = 3x1\n",
    "    \n",
    "    x = hadamard product (element-wise multiplication)\n",
    "    \n",
    "    [yHat - y]     [S'(Z2)]     [E]\n",
    "    [yHat - y]  x  [S'(Z2)]  =  [E] \n",
    "    [yHat - y]     [S'(Z2)]     [E] \n",
    "    \n",
    "    '''\n",
    "    l1_error = lastLayerError(np.dot(l0_activation, l1_weights), l1_activation, targets)\n",
    "    l0_error = hiddenLayerError(np.dot(features, l0_weights), l1_weights, l1_error)\n",
    "    \n",
    "    # Calculate cost gradient for weights\n",
    "    w1_gradient = costGradientWithRespectToWeights(l1_error, l0_activation)\n",
    "    w0_gradient = costGradientWithRespectToWeights(l0_error, features)\n",
    "\n",
    "    # Update weights based on our cost gradient\n",
    "    l0_weights -= learningRate * w0_gradient\n",
    "    l1_weights -= learningRate * w1_gradient\n",
    "    print l0_weights, l1_weights\n",
    "\n",
    "for i in range(10):\n",
    "    update_weights(features, l0_weights, l1_weights, .001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://scikit-neuralnetwork.readthedocs.io/en/latest/module_mlp.html#regressor\n",
    "from sknn.mlp import Regressor\n",
    "from sknn.mlp import Layer\n",
    "hiddenLayer = Layer(\"Sigmoid\", units=2)\n",
    "outputLayer = Layer(\"Linear\", units=1)\n",
    "nn = Regressor([hiddenLayer, outputLayer],learning_rule='sgd',learning_rate=.01,\n",
    "               batch_size=1,loss_type=\"mse\",debug=True,verbose=True,regularize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = get_inputs(-4,5)\n",
    "targets = get_targets(inputs)\n",
    "nn.fit(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = nn.predict(inputs)\n",
    "plt.plot(predictions)\n",
    "plt.plot(targets)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
